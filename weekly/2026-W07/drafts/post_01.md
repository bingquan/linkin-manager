### Don’t Regulate AI Models. Regulate AI Use

If efforts to regulate AI models are directionally correct, most teams are still optimizing the wrong bottleneck. The focus should shift from regulating the models themselves to controlling how these models are used in various contexts. This strategic pivot is necessary to address real-world risks effectively.

#### Why Model-Centric Regulation Fails

Efforts to regulate AI models, such as licensing "frontier" training runs or restricting open weights, promise control but deliver little practical benefit. Once released, model weights and code can spread globally at near-zero cost. Compliance efforts often bog down compliant firms in paperwork, while non-compliant actors easily circumvent these rules. Additionally, attempts to limit the publication of AI models may collide with free speech laws, particularly in the United States where software source code is considered protected expression.

#### A Practical Alternative: Regulate Use, Proportionate to Risk

A use-based regulatory regime classifies AI deployments by risk and scales obligations accordingly. Here’s a workable template:

1. **Baseline: General-Purpose Consumer Interaction**
   - Open-ended chat, creative writing, learning assistance, casual productivity.
   - Regulatory adherence: Clear AI disclosure at point of interaction, published acceptable-use policies, technical guardrails, and user-flagging mechanisms.

2. **Low-Risk Assistance**
   - Drafting, summarization, basic productivity.
   - Regulatory adherence: Simple disclosure, baseline data hygiene.

3. **Moderate-Risk Decision Support Affecting Individuals**
   - Hiring triage, benefits screening, loan prequalification.
   - Regulatory adherence: Documented risk assessment, human oversight, and an “AI bill of materials.”

4. **High-Impact Uses in Safety-Critical Contexts**
   - Clinical decision support, critical-infrastructure operations.
   - Regulatory adherence: Rigorous pre-deployment testing, continuous monitoring, incident reporting, and authorization.

5. **Hazardous Dual-Use Functions**
   - Tools to fabricate biometric voiceprints to defeat authentication.
   - Regulatory adherence: Confined to licensed facilities and verified operators; prohibition of unlawful capabilities.

#### Close the Loop at Real-World Choke Points

Regulators should focus enforcement at points of distribution (app stores and enterprise marketplaces), capability access (cloud and AI platforms), monetization (payment systems and ad networks), and risk transfer (insurers and contract counterparties). For high-risk uses, identity binding, capability gating, and tamper-evident logging are crucial. This approach creates market dynamics that incentivize compliance, ensuring that all players meet safety standards.

#### Systems Implication

By focusing on use cases and enforcing at critical choke points, the regulatory framework can create a scalable and effective system. This approach ensures that AI systems are safe and compliant at the points where they interact with users and institutions, reducing the likelihood of harmful outcomes. Technical anchors include clear disclosure requirements, rigorous testing protocols, and tamper-evident logging mechanisms.

#### Evaluative Judgment and Recommendation

The proposed use-based regulation is more practical and legally sustainable compared to model-centric approaches. It aligns with existing frameworks like the EU AI Act while addressing unique U.S. constitutional considerations. This method provides a balanced approach that promotes innovation while safeguarding against misuse.

#### Prompt Question

How can your organization implement a risk-based approach to AI use, ensuring compliance and safety at critical operational points?

---

[Link to Article](https://spectrum.ie
