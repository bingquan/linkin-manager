Don’t Regulate AI Models. Regulate AI Use looks strong on paper; production risk shows up one layer deeper.

Don’t Regulate AI Models. Regulate AI Use is worth attention, but only if we separate signal from framing.
Core claim: The public narrative overstates capability when evaluation scope is narrow.

Most discussion focuses on first-order performance metrics.
The practical question is what fails when assumptions shift: data regime, user behavior, and adversarial pressure.

Technical anchor: benchmark choice and metric definition can hide failure transfer to real tasks.

Systems implication: organizations that separate model quality from governance quality will misprice operational risk, especially when incentives reward launch speed over eval depth.

Judgment: this is useful progress, but teams should delay broad rollout until they can reproduce results under their own threat model and monitoring constraints.

Prompt question: What evidence would change your deployment decision this quarter?
